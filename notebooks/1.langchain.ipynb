{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概要\n",
    " 1. language modelsを使ってみる\n",
    " 2. OutputParsersを使ってみる\n",
    " 3. PromptTemplateを使ってみる\n",
    " 4. LCELを使ってみる\n",
    " 5. 会話履歴を使ってみる\n",
    "\n",
    "### リンク\n",
    " - <a href=\"https://python.langchain.com/v0.2/docs/tutorials/llm_chain/\" target=_blank>Langchain Tutorial</a>\n",
    " - <a href=\"https://pypi.org/project/langchain/#history\" target=_black>Langchain Version</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import (\n",
    "    AzureOpenAIEmbeddings,\n",
    "    OpenAIEmbeddings,\n",
    "    AzureChatOpenAI,\n",
    "    ChatOpenAI\n",
    ")\n",
    "from langchain_core.messages import (\n",
    "    HumanMessage, \n",
    "    AIMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. language modelsを使ってみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-1. langchainを使わない場合\n",
    "\n",
    " - <a href=\"https://learn.microsoft.com/ja-jp/azure/ai-services/openai/chatgpt-quickstart?tabs=command-line%2Cpython-new&pivots=programming-language-python\" target=_blank>モデルの実行</a>\n",
    " - <a href=\"https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-deprecation#latest-ga-api-release\" target=_blank>Azure API Versionの確認</a>\n",
    " - <a href=\"https://platform.openai.com/docs/models\" target=_blank>OpenAIのモデルリスト</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import (\n",
    "    OpenAI,\n",
    "    AzureOpenAI\n",
    ")\n",
    "# Azureの場合\n",
    "if os.getenv('AZURE_OPENAI_API_KEY') != \"\":\n",
    "    client = AzureOpenAI(api_version=\"2024-06-01\")\n",
    "    model_name = \"chat\" # Azureでデプロイしたモデル名\n",
    "# OpenAIの場合\n",
    "elif os.getenv('OPENAI_API_KEY') != \"\":\n",
    "    client = OpenAI()\n",
    "    model_name = \"gpt-4\" # OpenAIのモデル名 \n",
    "else:\n",
    "    print(\"APIKeyの設定を確認してください\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=model_name, \n",
    "    # ロールはsystem, user, assistantを指定する。contentはユーザーが入力したテキスト。\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"あなたは役に立つアシスタントだ。\"},\n",
    "        {\"role\": \"user\", \"content\": \"Azure OpenAIはcustomer managed keysをサポートしていますか？\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"はい、customer managed keysはAzure OpenAIでサポートされています。\"},\n",
    "        {\"role\": \"user\", \"content\": \"他のAzure AIサービスもこれをサポートしていますか？\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-2. langchainを使う場合\n",
    "\n",
    "<a href=\"https://python.langchain.com/v0.2/docs/tutorials/llm_chain/#using-language-models\" target=_blank>モデルの実行</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMの定義\n",
    "model = None\n",
    "# Azureの場合\n",
    "if os.getenv('AZURE_OPENAI_API_KEY') != \"\":\n",
    "    model = AzureChatOpenAI(\n",
    "        azure_deployment=\"chat\",\n",
    "        openai_api_version=\"2024-06-01\"\n",
    "    )\n",
    "# OpenAIの場合\n",
    "elif os.getenv('OPENAI_API_KEY') != \"\":\n",
    "    model = ChatOpenAI(model=\"gpt-4\")\n",
    "else:\n",
    "    print(\"APIKeyの設定を確認してください\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchainでは、SystemMessage, HumanMessage, AIMessageの3つのメッセージタイプを使用します。\n",
    "messages = [\n",
    "    SystemMessage(content=\"あなたは役に立つアシスタントだ。\"),\n",
    "    HumanMessage(content=\"Azure OpenAIはcustomer managed keysをサポートしていますか？\"),\n",
    "    AIMessage(content=\"はい、customer managed keysはAzure OpenAIでサポートされています。\"),\n",
    "    HumanMessage(content=\"他のAzure AIサービスもこれをサポートしていますか？\")\n",
    "]\n",
    "\n",
    "result = model.invoke(messages)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 補足：会話スタイルとパラメータ設定\n",
    "\n",
    "| 会話スタイル | Temperature | Top P | 説明 |\n",
    "|--------------|--------------|-------|------|\n",
    "| **創造的に** | 0.7 - 1.0    | 0.9   | 高いtemperatureはモデルが多様な出力を生成しやすくし、創造的な応答を促します。Top Pも高めに設定して、広範な単語選択を可能にします。 |\n",
    "| **バランスよく** | 0.5 - 0.7    | 0.8   | 中程度のtemperatureはバランスの取れた応答を生成し、創造性と一貫性のバランスを保ちます。Top Pも中程度に設定します。 |\n",
    "| **厳密に** | 0.0 - 0.3    | 0.7   | 低いtemperatureはモデルが最も確率の高い単語を選びやすくし、厳密で一貫性のある応答を生成します。Top Pも低めに設定して、確定的な出力を促します。 |\n",
    "\n",
    "``` Python\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "        azure_deployment=\"chat\",\n",
    "        openai_api_version=\"2024-06-01\",\n",
    "        temperature=0.5,\n",
    "        top_p=1.0\n",
    "    )\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. OutputParsersを使ってみる\n",
    "AIの回答のフォーマットを指定して取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()\n",
    "parser.invoke(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contentからも取得できる\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. PromptTemplateを使ってみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-1. PromptTemplateを使わない場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"イタリアの言語に翻訳してください\"),\n",
    "    HumanMessage(content=\"こんにちは\"),\n",
    "]\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"英語の言語に翻訳してください\"),\n",
    "    HumanMessage(content=\"こんばんわ\"),\n",
    "]\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この2つの例を見ると、例えば\n",
    " - イタリア、英語などの言語を変更したい\n",
    " - 翻訳する対象（Humanmessage）を変更したい\n",
    "\n",
    "ということもある。  \n",
    "\n",
    "これを解決するのが、PromptTemplateの機能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-2. PromptTemplateを使う場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# プロンプトの定義\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"{language}の言語に翻訳してください:\"), \n",
    "        (\"user\", \"{text}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辞書型で{}に入れる値を指定\n",
    "prompt = prompt_template.invoke({\"language\": \"italian\", \"text\": \"こんにちは\"})\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# メッセージだけを抜粋\n",
    "prompt.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Promptを使ってみる\n",
    "result = model.invoke(prompt)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. LCELを使ってみる\n",
    " 今まで実行してきたように、Promtを設定 > モデルから回答取得 > 出力を成形と処理が続く  \n",
    " これをLCELと表記で、続けて記述することができます。  \n",
    " Chainという考え方になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-1. LCEL(Chain)を使わない場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PromptTemplateでPromptを作成\n",
    "prompt = prompt_template.invoke({\"language\": \"italian\", \"text\": \"こんにちは\"})\n",
    "# modelにPromptを渡して結果を取得\n",
    "response = model.invoke(prompt)\n",
    "# 結果を文字列に変換\n",
    "result = parser.invoke(response)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-2. LCEL(Chain)を使わない場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |を使って、PromptTemplate, ChatOpenAI, StrOutputParserをつなげる\n",
    "chain = prompt_template | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# あとは、chainを使って、結果を取得\n",
    "chain.invoke({\"language\": \"italian\", \"text\": \"こんにちは\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 会話履歴を使ってみる\n",
    "チャットでは会話履歴が大事になってきます。  \n",
    "会話履歴の取り扱いについて習得していきましょう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-1. 会話履歴の必要性\n",
    "なぜ、会話の履歴が必要か確認していきます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1つ目のメッセージをいれてみます\n",
    "model.invoke([HumanMessage(content=\"こんにちは、もものきです\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2つ目のメッセージをいれてみます\n",
    "model.invoke([HumanMessage(content=\"私の名前を覚えていますか？\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように、1つずつメッセージを入れても、modelは適切な回答をしてくれません。  \n",
    "過去の会話も続けて、modelに入れるとチャットのようなつづけた会話ができます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"こんにちは、もものきです\"),\n",
    "        AIMessage(content='こんにちは、もものきさん。ご用件はありますか？'),\n",
    "        HumanMessage(content=\"私の名前を覚えていますか？\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-2. 会話履歴 その1(<a href=\"https://python.langchain.com/v0.2/docs/tutorials/chatbot/#message-history\" target=_blank>v0.2 公式ドキュメント</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# 会話履歴置き場\n",
    "store = {}\n",
    "\n",
    "# セッションIDを指定して履歴を取得\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# 履歴を取得するためのRunnableWithMessageHistoryを作成\n",
    "with_message_history = RunnableWithMessageHistory(model, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session_idのConfigを作成\n",
    "# session_idは新しい会話を開始するたびに変更したりすると、会話ごとの履歴を保持することができます。\n",
    "config = {\"configurable\": {\"session_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# message_historyを使って1つ目の質問を投げてみる\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"こんにちは、もものきです\")],\n",
    "    config=config,\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# message_historyを使って2つ目の質問を投げてみる\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"私の名前を覚えていますか？\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-3. 会話履歴その2 ConversationBufferMemoryなど\n",
    "以前の方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "chain = ConversationChain(\n",
    "     llm=model,\n",
    "     memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run([HumanMessage(content=\"こんにちは、もものきです\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run([HumanMessage(content=\"私の名前を覚えていますか？\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-4. 会話履歴その3 スクラッチ\n",
    "langchainは便利な機能がある一方、後ろで何をしているかわかりにくい場合もあります。  \n",
    "また、更新が多いライブラリであり、方法が変わっていきます。  \n",
    "次のコードでメッセージ履歴のイメージを把握しましょう！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 会話履歴\n",
    "messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(HumanMessage(content=\"こんにちは、もものきです\"))\n",
    "response = model.invoke(messages)\n",
    "messages.append(AIMessage(content=response.content))\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(HumanMessage(content=\"私の名前を覚えていますか？\"))\n",
    "response = model.invoke(messages)\n",
    "messages.append(AIMessage(content=response.content))\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-5. 会話履歴管理\n",
    "  \n",
    "会話を続けると、履歴が長くなり、modelに入れるトークン数を超えたり、トークン数によって費用が決まるため、費用がかさむ場合があります。  \n",
    "  \n",
    "そのため、会話履歴は管理する必要があります。  \n",
    "  \n",
    "5-3. 会話履歴その2で紹介したMemoryには、ConversationSummaryMemory, ConversationBufferWindowMemoryなどで会話履歴のトークン数を調整する方法があります。  \n",
    "  \n",
    "\n",
    "直近のドキュメントでは、trim_messagesなどがありますが、シンプルに配列で調整してもよい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "]\n",
    "messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_message = [\n",
    "    messages[0], # system message\n",
    "    messages[-3], # human message\n",
    "    messages[-2], # ai message\n",
    "    messages[-1] # 最新 human message\n",
    "]\n",
    "new_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "new_message = [messages[0]]\n",
    "new_message.extend(messages[-k:])\n",
    "new_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(new_message)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
